## 4.1 성능 테스트 유형
테스트로 확인하고 싶은 정량적 질문 리스트와 그 테스트가 대상 애플리케이션 입장에서 중요한 이유를 간단히 적어 성능 테스트를 기획하자.

일반적인 성능 테스트

### 4.1.1 지연 테스트
* 지연 : 하나의 트랜잭션을 처리하고 그 결과를 반대편 끝에서 바라볼 때까지 소요된 시간(종단시간)
> '고객이 트랜잭션(또는 페이지 로딩)을 얼마나 오래 참고 기다려야 하는지' 측정하는 테스트
* 종단 트랜잭션에 걸리는 시간은 ?
* 목적 : 유저 경험을 직접 개선하거나 서비스 수준 협약서 조항을 이행하는 것

### 4.1.2 처리율 테스트
* 처리율 : 시스템이 수행 가능한 작업 비율을 나타낸 지표
> 지연을 모니터링하면서 테스트한다. 이때 지연 분포가 변하는 시점, 즉 한계점이 바로 '최대 처리율'이다.
* 현재 시스템이 처리 가능한 동시 트랜잭션 개수는 ?
* 목표 : 시스템 성능이 급락하기 전, '최대 처리율' 수치를 측정하는 것

### 4.1.3 부하 테스트
> 임계치 한계에 도달 할 때까지 시스템의 부하를 지속적으로 지속적으로 증가시켜 시스템을 테스트하는 것
* 특정 부하를 시스템이 감당할 수 있는가 ?
* 신규 고객을 유치하거나 새로운 시장에 진출하기 전, 애플리케이션 트래픽이 상당할 것으로 예상되는 특정 비즈니스 이벤트, 예를 들어 광고 캠페인, 바이럴 콘텐츠, 에 대비하기 위한 부하테스트를 수행할 수 있다.

### 4.1.4 스트레스 테스트
> 시스템 여력이 어느 정도인지 알아보는 수단이다.
* 이 시스템의 한계점은 어디까지인가 ?

### 4.1.5 내구 테스트
> 평균 또는 그보다 높은 사용률로 시스템에 일정 부하를 계속 주며 모니터링하다가 갑자기 리소스가 고갈되거나 시스템이 깨지는 지점을 찾는다.
* 시스템을 장시간 실행할 경우 성능 이상 증상이 나타나는가 ?
* 빠른 응답을 요구하는 시스템에서 많이 한다.

### 4.1.6 용량 계획 테스트
* 스트레스 테스트 : 현재 시스템이 어느 정도 부하를 버틸 수 있는지 알아보는 것
> '업그레이드'한 시스템이 어느 정도 부하를 감당할 수 있을지 미리 내다보는 것

### 4.1.7 저하 테스트

* 저하 : 지연이 증가하는 양상
> 복원 테스트 하나만 생각하면 된다. 기본적으로 평상시 운영 환경과 동등한 수준의 부하를 시스템에 가하는 도중, 어떤 컴포넌트나 전체 서브시스템이 갑자기 능력을 상실하는 시점에 벌어지는 일들을 확인한다.
* 대표적인 하위 유형으로는 카오스 멍키가 있다.\
    : 복원성 있는 아키텍처에서는 어느 한 컴포넌트가 잘못되어도 다른 컴포넌트까지 연쇄적으로 무너뜨리면서 전체 시스템에 부정적 영향을 끼치지 않는다.

## 4.2 기본 베스트 프랙티스

성능 튜닝시 기본 3가지 원칙에 따라 결정한다.
* 나의 관심사가 무엇인지 식별하고 그 측정 방법을 고민한다.
* 최적화하기 용이한 부분이 아니라, 중요한 부분을 최적화한다.
* 중요한 관심사를 먼저 다룬다.

### 4.2.1 하향식 성능
> 전체 애플리케이션의 성능 양상부터 먼저 알아보는 접근 방식이다.

* 성과를 극대화하기 위해서는 먼저 테스트팀이 테스트 환경을 구축한 다음, 무엇을 측정하고 무엇을 최적화해야 하는지, 또 성능 활동을 전체 소프트웨어 개발 주기에서 어떻게 병행해야 하는지, 전 팀원이 명확하게 이해해야 한다.

### 4.2.2 테스트 환경 구축

* 테스트 환경 구축은 테스트 팀이 가장 먼저해야하는 일이며, 테스트 환경은 가급적 모든 면에서 운영 환경과 똑같아야 한다. 
    * 애플리케이션 서버, 웹 서버, DB, 로드 밸런서, 네트워크 방화벽 등도 맞추어야 한다.
    * 운용 중인 각종 서비스도 목업 형태로 반영돼야 한다.
* 운영 환경이 반영되지 않은 QA 환경에서 성능 테스트하는 것은 리스크가 매우 크다.

### 4.2.3 성능 요건 식별

* 전체 시스템 성능은 애플리케이션 코드뿐만 아니라 컨테이너, OS, 하드웨어 모두 나름대로 영향을 끼친다.
* 성능을 평가하는 지표는 코드 관점에서만 생각해서도 안 되고, 시스템을 전체적으로 바라보며 고객과 경영진에게 중요한 측정값을 고려해야 한다.
* 성능 비기능 요건 (NFR) : 최적화하려는 핵심 지표
    * 응답 시간 : 평균 응답 시간을 30% 줄인다.
    * 처리율 : 기존 하드웨어 처리율을 5배 높일 수 있게 시스템을 개선한다.

### 4.2.4 자바의 특정 이슈
* JVM 에는 성능 엔지니어가 잘 이해하고 주의 깊게 살펴야 할 복잡한 부분들이 있다.
* 메모리 영역의 동적 튜닝 등 JVM 특유의 다이내믹한 자기 관리 기능이 추가되면서 복잡해졌기 때문이다.
* 특히 JIT 컴파일은 중요한 부분이라서 유심히 살펴야 한다.
* 성능과 관려해서 어떤 메서드가 컴파일 중인지 로그를 남겨 살피고 핵심 코드 경로상의 중요 메서드가 잘 컴파일되고 있는지 확인하는 것이 중요하다.
    * JVM은 어떤 메서드를 JIT 컴파일해서 최적화한 기계어로 변환할지 분석한다.
    * JIT 컴파일 안하기로 결정된 메서드
        1. 자주 실행되는 메서드가 아니다.
        2. 메서드가 너무 크고 복잡해서 컴파일 분석을 할 수 없다.

### 4.2.5 SDLC 일부로 성능 테스트 수행하기
* 성능 테스트를 전체 SDLC(소프트웨어 개발 수명주기)의 일부로 수행해야 한다.

## 4.3 성능 안티패턴 개요
* 안티패턴 : 사람들이 수많은 프로젝트를 수행하면서 밝혀낸, 소프트웨어 프로젝트 또는 팀의 좋지 않은 패턴

캘리 플리첼의 "왜 개발자는 잘못된 기술 선택을 밥 먹듯이 하나" 의 5가지 원인 (안티패턴)

1. 지루함

    : 자기 역할에 지루함을 느껴 필요하지 않은 기술을 사용하거나 억지로 새로운 기술을 도입한다. 일례로 Collections.sort() 한 줄이면 될 것을 직접 정렬 알고리즘을 구현해 필요 이상으로 복잡하게 코딩을 하거나, 지금껏 알려지지 않은 기술로 컴포넌트를 제작하기도 한다.

2. 이력서 부풀리기

    : 자신의 이력서를 부풀리기 위해 프로젝트에 불필요한 기술을 덧붙일 수 있다. 개발자의 지루함, 이력서 부풀리기 탓에 불필요한 기술을 덧댄 결과, 개발자가 이직한 후에도 아주 오랜 기간동안 시스템에 지대한 영향을 끼치게 된다.

3. 또래 압박

    : 팀원들이 기술을 결정할 때 관심사를 분명히 밝히지 않고 충분한 논의없이 진행하면 쓴 결과를 맛보기 쉽다.

4. 이해 부족

    : 지금 사용하는 툴의 기능도 온전히 알지 못하면서 무턱대고 새로운 툴로 문제를 해결하는 것은 좋지 않다. 기술 복잡도를 높이는 것과 현재 툴로도 할 수 있는 것 사이의 균형을 잘 맞추어야 한다.

5. 오해와 있지도 않은 문제

    : 문제 자체가 무엇인지 제대로 이해하지 못한 채 오로지 기술을 이용해 문제를 해결하는 상황

$ \rightarrow $ 이러한 안티패턴을 예방하기 위해서는 팀원 모두 참여해서 기술 이슈를 활발히 공유하고, 뭔가 불분명하다 싶으면 먼저 사실에 근거한 증거를 수집하고 프로토타입을 만들어 조사해야 한다.

## 4.4 성능 안티패턴 카탈로그

1. 화려함에 사로잡히다.
* 증상 : 레거시 코드보다 신기술의 작동 원리를 연구하는 것이 더 재밌고, 코드도 관리하기 쉽다고 생각하여 최신의 멋진 기술을 튜닝타깃으로 정한다.
* 현실 : 
* 진단 : 
* 처방 : 측정을 해보고 진짜 성능 병목점을 찾으세요.

2. 단순함에 사로잡히다.

* 증상 : 전체를 확인하지 않고 무작정 시스템에서 제일 간단한 부분만 파고든다.
* 현실 : 다양한 시스템 컴포넌트에 대해 지식 공유를 하지 않고 짝 프로그래밍을 안 한 결과, 독보적인 전문가만 양산된다.
* 진단 : 개발자가 시스템에서 자기가 익숙한 부분만 프로파일링하려고 하기 때문에 발생한다.
* 처방
    * 측정을 해보고 진짜 성능 병목점을 찾으세요. 
    * 본인이 익숙하지 않은 컴포넌트에 문제가 생기면 잘 아는 전문가에게 도움을 청하세요. 
    * 개발자가 전체 시스템 컴포넌트를 고루 이해하도록 독려하세요.

3. 성능 튜닝 도사

* 증상 : 전체를 확인하지 않고 무작정 시스템에서 제일 간단한 부분만 파고든다.
* 진단 : 개발자가 시스템에서 자기가 익숙한 부분만 프로파일링하려고 하기 때문에 발생한다.
* 처방
    * 측정을 해보고 진짜 성능 병목점을 찾으세요. 
    * 본인이 익숙하지 않은 컴포넌트에 문제가 생기면 잘 아는 전문가에게 도움을 청하세요. 
    * 개발자가 전체 시스템 컴포넌트를 고루 이해하도록 독려하세요.

4. 민간 튜닝

* 증상 : 서비스 중 발생한 성능 문제를 해결하려는 한 팀원이 웹사이트에서 '마법'의 설정 매개변수를 발견한다. 그리고 테스트도 안 해보고 곧장 마법의 매개변수를 운영 서버에 적용한다.
* 현실 : 어떤 시스템에선 통했을지 모르지만 다른 시스템에 적용해도 효험이 있을지 모를 일이다.
* 진단 : 성능 팁은 유통 기한이 짧아 금세 낡은 유물이 되기 쉽다. 나중에 소프트웨어/플랫폼이 업데이트되고 새로운 해결책이 등장하면 무용지물이 되기 쉽다. 또한 자바 성능은 여러 가지 잡다한 요인이 영향을 미치는 맥락에서 발생하기 때문에 전후 맥락을 벗겨낸 나머지만 갖고는 거의 추론할 수가 없다.
* 처방
    * 충분히 검증된 것들만 적용하세요.
    * 매개 변수를 UAT 에서 시험해보세요. 어떤 변화라도 철저히 검증하고 효용을 프로파일링하는 일이 중요합니다.
    * 다른 개발자나 운영 요원, 데브옵스팀과 함께 설정 문제를 리뷰하고 검토합니다.

5. 안되면 조상 탓

* 증상 : 정작 이슈와 아무 상관 없는 특정 컴포넌트를 문제 삼는다.
* 현실 : 충분히 분석도 안 해보고 성급한 결론을 내린다. 진짜 원인을 밝히려면 숲을 봐야 하는데 팀원들은 그럴 마음이 없다.
* 진단 : 아무래도 뭔가 새로 조사하는 것보다는 보통 문제를 많이 일으키는 곳을 지목하는 게 편하기 때문에 이런 현상이 발생한다. 하이버네이트가 이러한 안티패턴의 좋은 예시인데, 과거 하이버네이트로 고생을 많이 한 경험이 있는 팀은 일단 문제가 발생했을 때 이 기술을 비난한다. 하지만 정작 문제는 하부 쿼리 문제, 부적절한 인덱스 사용 등 다른 곳에 있을 가능성이 크다. 따라서 정확한 원인을 밝히려면 프로파일링이 필수이다.
* 처방
    * 성급한 결론을 내리고픈 욕망에 굴하지 마세요.
    * 정상적으로 분석을 하세요.
    * 분석 결과를 모든 이해관계자와 의논하세요.

6. 숲을 못 보고 나무만 보다.

* 증상 : 전체적인 변경 영향도를 완전히 파악하지 않은 채 일단 그냥 변경을 해보거나 애플리케이션의 국소적인 부분만 프로파일링한다.
* 현실 : 변경 영향도를 완전히 이해한 사람이 아무도 없다. 마이크로벤치마킹 때문에 빚어질 전체 시스템 영향도를 파악하지 않는다.
* 진단 : 
* 처방
    * 성급한 결론을 내리고픈 욕망에 굴하지 마세요.
    * 정상적으로 분석을 하세요.
    * 분석 결과를 모든 이해관계자와 의논하세요.

7. 내 데스크톱이 UAT

* 증상 : 
* 현실 : UAT 환경이 운영계와 달라서 서비스가 중단되는 사태가 벌어지면 장비 몇 대 추가하는 비용보다 훨씬 더 값비싼 대가를 치르게 된다.
* 진단 : 
* 처방
    * 서비스 중단 비용과 고객 이탈로 인한 기회비용을 잘 따져보세요.
    * 운영 환경과 동일한 UAT 환경을 구입하세요.
    
8. 운영 데이터처럼 만들기는 어려워

* 증상 : 사람들이 운영계와 유사한 데이터를 나타내고자 할 때 빠지는 함정이다. 편의를 위해 테스트 과정을 단순화하여 유의미한 결과를 얻는데 한계가 있다.
* 현실 : UAT 에서 정확한 결과를 얻으려면 운영계 데이터와 최대한 맞추어야 한다.
* 진단 :  운영 데이터와는 전혀 다른 모습이라도 이 정도면 되겠다, 싶은 데이터로 테스트할 때 진짜 결함이나 누락된 부분이 드러나는 경우도 있지만, 그릇된 판단을 내릴 소지가 다분한다.
* 처방
    * 데이터 도메인 전문가에게 컨설팅을 받고 운영 데이터를 UAT로 다시 이전하는 프로세스에 시간과 노력을 투자하세요.
    * 다수의 고객이 몰리고 엄청난 트랜잭션이 예상되는 서비스는 출시 전 철저히 준비하세요.
