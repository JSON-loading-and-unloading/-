<h1>성능 테스트 유형</h1>
※성능 테스트를 기획하는 요령<br>
 => 테스트로 확인하고 싶은 정량적 질문 리스트와 그 테스트가 대상 애플리케이션 입장에서 중요한 이유를 적어보기<br><br>

 <h3>지연 테스트</h3>

  - 고객이 트랜잭션(페이지 로딩)을 얼마나 오래 참고 기다려야 하는지 측정한 시스템 수치

 <h3>처리율 테스트</h3>

  - 시스템 성능이 급락하기 직전, 최대 처리율 수치를 측정하는 것이 목표
  - 지연 분포가 갑자기 변하는 시점, 즉 사실한 한계점이 바로 최대 처리율이다.

 <h3>부하 테스트</h3>

 - 시스템이 이 정도 부하를 견딜 수 있을까?, 없을까? 하는 질문에 답을 구하는 과정
   

 <h3>스트레스 테스트</h3>

 - 시스템 여력이 어느 정도인지 알아보는 수단
 
 <h3>내부 테스트</h3>

 - 메모리 누수, 캐시 오염, 메모리 단편화 등 시간이 지나고 드러나는 문제점이 나타나는 것을 측정
 - 평균 사용률로 시스템에 일정 부하를 계속 주며 모니터링하다가 갑자기 리소스가 고갈되거나 시스템이 깨지는 지점을 찾음

 <h3>용량 계획 테스트</h3>

 - 업그레이드한 시스템이 어느 정도 부하를 감당할 수 있는지 미리 내다보는 것

 <h3>저하 테스트</h3>

 - 운영 환경과 동등한 수준의 부하를 시스템에 가하는 도중, 어떤 컴포넌트나 전체 서브시스템이 갑작기 능력을 상실하는 시점에 벌어지는 일들을 확인
 - 트랜잭션 지연 분포을 눈여겨 봐야할 측정값

<h1>기본 베스트 프랙티스</h1>

1. 하향식 성능<br>
   => 테스트 환경 구축 이후 무엇을 측정하고 무엇을 최적화해야 하는지, 또 성능 활동을 소프트웨어 개발 주기에서 어떻게 병행해야 하는지 이해<br>
2. 테스트 환경 구축<br>
   =>웹 서버, DB, 로드 밸런서, 네트워크 방화벽 등도 운영환경과 똑같이 맞춰야한다.<br>
3. 성능 요건 식별<br>
   => 성능을 평가하는 지표는 코드 관점에서만 생각해서도 안 되고, 시스템을 전체적으로 바라봐야함.<br>
4. 자바의 특정 이슈<br>
   => 최신 JVM은 어떤 메서드를 JIT 컴파일해서 최적화한 기계어로 변환할지 분석함<br>
JIT 컴파일 안 하기로 결정된 메서드<br>
1. jit컴파일할 정도로 자주 실행되는 메서드가 아니다.<br>
2. 메서드가 너무 크고 복잡해서 도저히 컴파일 분석을 할 수 없다.<br>
5. SDLC 일부로 성능 테스트 수행하기<br>
   => 성능 테스트를 전체 SDlc의 일부로서 수행하며, 특히 성능 회귀 테스트를 상시 수행한다.<br>

   성능 회귀 테스트 : 소프트웨어의 성능이 이전 버전과 비교하여 저하되는지를 확인하기 위해 수행되는 테스트입니다.<br>
   
<h1>성능 안티패턴 개요</h1>

개발자들의 잘못된 기술 선택 이유<br>
1. 지루함
2. 이력서 부풀리기
3. 또래 압박
4. 이해 부족
5. 오해와 있지도 않은 문제
   
 <h1>성능 안티패턴 카탈로그</h1>

 <h4>화려함에 사로잡히다</h4>

증상 : 개발자는 새로 나온 애플리케이션 컴포넌트를 찾아 헤매는 강박에 사로잡힘<br>
처방 : <br>
   - 측정을 해보고 진짜 성능 병목점을 찾는다.<br>
   - 새 컴포넌트는 전후로 충분한 로그를 남긴다.<br>
   - 베스트 프랙티스 및 단순화한 메모리 데모를 참조한다.<br>
   - 팀원들이 새 기술을 이해하도록 독려하고 팀 차원의 베스트 프랙티스 수준을 정한다.<br>
 <h4>단순함에 사로잡히다</h4>
 증상 : 객관적으로 아픈 부위를 들추려 하지 않고 무작정 시스템에서 제일 간단한 부분만 파고 든다.<br>
 처방 : <br>
     - 측정을 해보고 진짜 성능 병목점을 찾아라<br>
     - 본인이 익숙지 않는 컴포넌트에 문제가 생기면 잘 아는 전문가에게 도움을 청한다.<br>
     - 개발자가 전체 시스템 컴포넌트를 고루 이해하도록 독려한다.<br>
 
  <h4>성능 튜닝 도사</h4>

  증상 : 천재 개발자를 채용하고 무리한 요구를 한다<br>
  처방 : <br>
      - 새로 채용된 팀내 전문가가 다른 팀원들과 지식을 공유하고 팀워크를 유지한다.<br>
      
  <h4>민간 튜닝</h4>

  증상 : 서비스 중 발생한 성능 문제를 해결하려고 안간힘들 쓰던 중, 한 팀원이 웹사이트에서 '마법'의 설정 매개변수를 발견함.<br>
  처방 : <br>
     - 시스템의 가장 중요한 부분에 직접 영향을 미치는 기술은 확실히 파악하고 충분히 검증된 것들만 적용한다.<br>
     - 다른 개발자나 운영요원, 데브옵스팀과 함께 설정 문제를 리뷰하고 토의한다.<br>
     - 매개변수를 UAT에서 시험해본다.<br>
   
  
  <h4>안되면 조상 탓</h4>
  증상 : 정작 이슈와는 아무 상관도 없는 특정 컴포넌트를 문제 삼는다.<br>
  처방 : <br>
      - 성급한 결론을 내리고픈 욕망에 굴하지 않는다.<br>
      - 정상적으로 분석을 한다.<br>
      - 분석 결과를 모든 이해관계자와 의논한다.<br>


<h4>숲을 못 보고 나무만 본다</h4>
증상 : 전체적인 변경 영향도를 완전히 파악하지 않은 채 일단 그냥 변경을 해보거나 애플리케이션의 국소적인 부분만 프로파일링한다.<br>
처방 : <br>
   스위피 변경하기 전 절자<br>
      1. 운영계 성능 지표를 측정한다.<br>
      2. UAT 에서 한 번에 스위치 하나씩 변경<br>
      3. 스트레스를 받는 지점이 UAT와 운영계가 동일하지 확인<br>
      4. 운영계에서 일반적인 부하를 나타내는 테스트 데이터를 확보<br>
      5. UAT에서 스위치를 변경하며 테스트<br>
      6. UAT에서 다시 테스트<br>
      7. 추론한 내용을 다른 사람에게 재검토 요청<br>
      8. 내린 결론을 다른 사람과 공유<br>

<h4>내 데스크톱이 UAT</h4>
증상 : 개발자 개인 장비는 대부분 운영계에 배포되는 작은 서버보다 성능이 더 좋다.<br>
처방 : <br>
     - 서비스 중단 비용과 고객 이탈로 인한 기회비용을 잘 따져본다.<br>
     - 운영환경과 동일한 UAT환경을 구입한다.<br>
     
<h4>운영 데이터처럼 만들기는 어려워</h4>

증상 : 데이터라이트는 사람들이 운영계와 유사한 데이터를 나타내고자 할 때 빠지는 함정.<br>
    잘못된 UAT 전략<br>
    1. 테스트 편의상, 하루에 오가는 메세지 중 작은 한 부분만 포착해서 UAT에서 전부 돌려본다.<br>
      => 시스템에서 나타나는 버스트 양상과 다른 시장에서 거래옵션이 열리기 전 특정 시장에서 더 많은 선물 거래가 일어나서 발생한은 웜업현상도 포착할 수 없다.<br>
    2. 테스트 편의상 단언 시 단순한 겂만 사용하도록 거래, 옵션 데이터를 업데이트한다.<br>
      => 데이터 현실성이 떨어진다. <br>
    3. 작업 편의상 모든 값을 한꺼번에 시스템에 밀어 넣는다.<br>
      => 데이터 적재 비율이 달라질 때 나타나는 핵심적인 웜업과 최적화를 놓치게 된다.<br>
처방 : <br>
     - 데이터 도메인 전문가에게 컨설팅을 받고 운영 데이터를 UAT로 다시 이전하는 프로세스에 시간과 노력을 투자한다.<br>
     - 다수의 고객이 몰리고 엄청난 트랜잭션이 예상되는 서비스는 출시 전 철저히 준비한다.<br>

<h1>인지 편향과 성능 테스트</h1>

     
<h4>환원주의</h4>
시스템을 아주 작은 조각으로 나누어 그 구성 파트를 이해하면 전체 시스템도 다 이해할 수 있다는 분석주의적 사고방식이다.<br>
=> 복잡한 시스템은 힘들다. => 단순히 구성 파타를 합한 것보다 시스템을 전체로서 바라봐야 문제의 원인을 찾을 수 있다.<br>
<h4>확증 편향</h4>
성능 면에서 중대한 문제를 초래하거나, 애플리케이션을 주관적으로 바라보게 한다.<br>
=> 테스트 세트를 부실하게 선택하거나 테스트 결과를 통계적으로 건전하게 분석하지 않으면 확증 편향이 포로가 되기 쉽다.<br>


<h4>전운의 그림자</h4>
시스템이 예상대로 작동하지 않는 상황, 또는 아예 중단된 시간 중에 발현되는 편향<br>
원인<br>
  - 영향도를 분석해보지도 않고, 또 다른 사람에게 안 하고 시스템의 인프라를 변경<br>
  - 시스템이 의존하는 라이브러리를 변경<br>
  - 연중 가장 업무가 빠드스한 날에 처음 보는 버그나 경합 조건이 발생<br>
 => 실패 시나리오를 충분히 테스트 안 해본 상태로 오픈해서 로그도 제대로 남기지 않는 애플리케이션이 아직도 많다. 이런 상황이면 숙련된 엔지니어도 시스템 중단을 해결하려고 뭐라고 해야 한다는 막연한 느낌으로 서두르기만 하다가 절망에 빠진다.<br>

 
<h4>위험 편향</h4>
누구나 예전에 뭔가 바꿨더니 잘못됐던 경험을 한두 가지 갖고 있어서 가급적 위험을 감수하지 않으려고 한다.<br>
=> 위험 편향은 보통 애플리케이션 문제가 생겼을 때 제대로 학습하고 적절한 조치를 하지 못한 까닭에 더 고착화된다.<br>

<h4>엘스버그 역설</h4>

인간이 확률을 이해하는데 얼마나 서투른지 잘 보여주는 사례<br>
'알려지지 않는 미지의 것'보다 '알려진 기지의 것'을 추구하는 인간 본연의 욕구에 관한 역설을 주장<br>

<h2>부하 테스트</h2>

Jmeter를 활용한 부하테스트 해보기<br>
![세팅](https://github.com/JSON-loading-and-unloading/Optimizing-Java/assets/106163272/9b8872e4-50fa-4abc-b896-bcc9fe497181)
(세팅 이미지)<br>
![쓰레드 그룹](https://github.com/JSON-loading-and-unloading/Optimizing-Java/assets/106163272/96e646f8-99af-4d91-8a3b-6d4db7a268d3)
(테스트 설정 이미지)<br>
![결과 트리](https://github.com/JSON-loading-and-unloading/Optimizing-Java/assets/106163272/23c54535-8381-458c-a67f-519c4fdcefdb)
(결과 트리 이미지)<br>
![요약보고서](https://github.com/JSON-loading-and-unloading/Optimizing-Java/assets/106163272/c04057dc-034d-4cbd-8dad-369b84bc6d32)
(쓰레드 그룹)<br>
![tps](https://github.com/JSON-loading-and-unloading/Optimizing-Java/assets/106163272/070bab03-3512-45ed-9dc0-bef1037efcef)
( tps그래프 이미지)<br>





