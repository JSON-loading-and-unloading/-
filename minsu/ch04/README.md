# 4 성능 테스트 패턴 및 안티패턴

## 4.1 성능 테스트 유형
### 4.1.1 지연 테스트 (Latency test)
- 트랜잭션 (또는 페이지 로딩)에 걸리는 시간은?
- (이 테스트가 너무 명확해서) 다른 종류의 성능 테스트로 밝히려는 정량적 질문을 식별할 필요성을 흐릿하게 만들지 않도록 주의

### 4.1.2 처리율 테스트 (Throughput test)
- 현재 시스템에서 처리 가능한 동시 트랜잭션 개수는?
- 최대 처리율: 시스템 성능이 급락하기 직전으로, 지연 분포가 갑자기 변하는 시점(변곡점)이다.

### 4.1.3 부하 테스트 (Load test)
- 시스템이 어느정도 부하를 견딜 수 있을까?

### 4.1.4 스트레스 테스트 (Stress test)
- 이 시스템의 한계점은 어디까지일까?
- 목표: 최대 처리율의 지점과 그 시점의 부하 수준을 포착하는 것

### 4.1.5 내구성 테스트 (Endurance test)
- 시스템을 장시간 실행할 경우 성능 이상 증상이 나타나는가?
- 메모리 누수, 캐시 오염, 메모리 단편화 등은 시간이 지나고 나서야 문제점이 발생 -> 따라서 평균 사용률로 시스템에 일정 부하를 계속 주며 모니터링하다 갑자기 리소스가 고갈되거나 시스템이 깨지는 지점을 찾는다.
- 빠른 응답을 요구하는 시스템에서 많이 함

### 4.1.6 용량 계획 테스트 (Capacity planning test)
- 리소스를 추가한 만큼 시스템이 확장되는가?
- 스트레스 테스트는 '현재' 시스템을 대상으로 한 반면 용량 계획 테스트는 '업그레이드'한 시스템이 어느 정도 부하를 감당할 수 있을지 보는 것 -> 예정된 계획의 일부분을 실행하는 경우가 많음


### 4.1.7 저하 테스트 (Degradation test)
- 시스템이 부분적으로 실패할 경우 어떤 일이 벌어지나?
- 기본적으로 평상시 운영 환경과 동등한 수준의 부하를 시스템에 가하는 도중, 어떤 컴포넌트나 전체 서브시스템이 갑자기 능력을 상실하는 시점에 벌어지는 일들을 확인

## 4.2 기본 베스트 프랙티스
성능 튜닝 시 주안점을 두어야 할 부분은 다음 세 가지 원칙에 따라 결정
```
- 나의 관심사가 무엇인지 식별하고 그 측정 방법을 고민한다.  
- 최적화하기 용이한 부분이 아니라, 중요한 부분을 최적화한다.
- 중요한 관심사를 먼저 다룬다.
```
### 4.2.1 하향식 성능
> 전체 애플리케이션의 성능 양상부터 먼저 알아보는 접근 방식을 Top-down performance라고 한다.

### 4.2.2 테스트 환경 구축
- 테스트 환경은 가급적 모든 면에서 운영 환경과 똑같이 복제해야 함
- 애플리케이션 서버 뿐 아니라 웹 서버, DB, 로드 밸런서, 네트워크 방화벽 등도 맞추어야 함
- 운용 중인 각종 서비스도 성능 테스트 환경에 mock-up 형태로 반영돼야 함
- 성능 테스트 환경이 운영 환경과 많이 차이가 나면 실제 환경에서 어떤 일들이 일어날지 예측하기 어렵고 쓸모있는 결과를 얻어내기 힘듦
- 논클르우드 환경에서는 오히려 운영 환경과 유사한 성능 테스트 환경을 구축하기 비교적 수월했음 -> 클라우드 기술의 출현으로 최근에는 주문형 및 자동 확장 인프라 기술이 발전해서 운영 환경과 유사한 성능 테스트 환경 구축이 더 버거워짐 , 그러나 사용하지 않을 때는 테스트 환경을 간편하게 꺼둘 수 있다는 장점도 있음

### 4.2.3 성능 요건 식별
- 성능을 평가하는 지표는 코드 관점에서만 생각하면 안되고, 시스템을 전체적으로 바라보아야 함
- 성능 비기능 요건 (NFR) : 최적화하려는 핵심 지표

아래는 아주 명확한 목표이다.
```
- 95% 백분위 트랜잭션 시간을 100밀리초 줄인다.
- 기존 하드웨어 처리율을 5배 높일 수 있게 시스템을 개선한다.
- 평균 응답 시간을 30% 줄인다.
```

### 4.2.4 자바의 특정 이슈
- JVM에는 성능 엔지니어가 잘 이해하고 주의 깊에 살펴야 할 복잡한 부분이 있음 -> 메모리 영역의 동적 튜닝 등 JVM 특유의 다이내믹한 자가 관리 기능이 추가되면서 복잡해진 까닭
- 최신 JVM은 어떤 메서드를 JIT 컴파일해서 최적화한 기계어로 변환활지 분석하는데 JIT 컴파일을 안하기로 결정된 메서드는 다음 둘 중 하나이다.
```
- JIT 컴파일할 정도로 자주 실행되는 메서드가 아니다.
- 메서드가 너무 크고 복잡해서 도저히 컴파일 분석을 할 수 없다.
```

### 4.2.5 SDLC 일부로 성능 테스트 수행하기
- 성능 테스트를 전체 SDLC의 일부로서 수행하며, 특히 성능 회귀 테스트를 상시 수행하는 편이 낫다. -> 개발팀, 인프라팀이 서로 조율해서 어느 시점에 몇 버전 코드를 성능 테스트 환경에 배포할지 조정해야 한다.

## 4.3 성능 안티패턴 개요
> 안티패턴은 사람들이 수많은 프로젝트를 수행하면서 밝혀낸, 소프트웨어 프로젝트 또는 팀의 좋지 않은 패턴이다.

### 4.3.1 지루함
- 개발자의 지루함은 프로젝트에 해악을 끼칠 수 있음
    - Collections.sort() 한 줄이면 될 것을 직접 정렬 알고리즘을 구현해 필요 이상으로 복잡하게 코딩하기
    - 알려지지 않은 기술로 컴포넌트를 제작하거나, 맞지도 않은 유스케이스에 억지로 기술을 욱여넣는 등 여러 가지 방법으로 지루함을 표출하기도 함


### 4.3.2 이력서 부풀리기
- 위와 마찬가지로 프로젝트를 불필요한 방향으로 끌고 갈 수 있음


### 4.3.3 또래 압박
- 기술을 결정할 때 관심사를 분명히 밝히지 않고, 서로 충분한 논의 없이 진행하면 안됨  
- 제대로 사정을 따져보지도 않고 섣불리 중요한 결정을 내리지 말 것

### 4.3.4 이해 부족
- 지금 사용하는 툴의 기능도 온전히 알지 못하는데 무턱대고 새로운 툴로 문제를 해결하려고 하지 말 것  
     - 기술 복잡도를 높이는 것과 현재 툴로도 할 수 있는 것 사이의 균형을 잘 맞추어야 함
     - 예를 들어 하이버네이트는 도메인 객체 <-> DB 객체 변환을 단순화하는 절호의 방법처럼 보이지만 이해가 부족한 상태에서 하이버네이트를 너무 복잡하게 사용하게 되면 결국 운영 단계에서 회복 불가능한 중단 사태를 맞이할 수 있다.

### 4.3.5 오해가 있지도 않은 문제
- 문제 자체가 무엇인지 제대로 이해하지 못한 채 오로지 기술을 이용해서 문제를 해결하려는 개발자도 있음
   - 성능 지표를 수집/분석해야만 비로소 문제의 본질을 정확히 이해할 수 있음


## 4.4 성능 안티패턴 카탈로그
### 4.4.1 화려함에 사로잡히다.
- 증상: 최신의 멋진 기술을 튜닝 타깃으로 정한다.   
- 현실:
    - 애플리케이션을 측정하고 튜닝하려고 하지 않고 어떻게 해보면 되겠지 라고 생각함  
    - 신기술을 제대로 알지 못한 상태에서 문서도 보지 않고 어설프게 지레짐작만 함  
    - 신기술에 관한 온라인 예제는 보통 작은 규모의 전형적인 데이터셋만 다룸
- 진단: 자신의 능력을 증명하려는 욕구, 레거시 시스템이라고 간주한 것에 매이지 않으려는 마음으로 오로지 최신의 '뜨고 있는' 기술을 숭배한다. -> 본인이 잘 모르면 다른 사람에게 자문을 구하기!
- 처방:
    - 측정을 해보고 진짜 성능 병목점을 찾기
    - 새 컴포넌트의 전후로 충분한 로그를 남기기
    - 베스트 프랙티스 및 단순화한 데모를 참조하기
    - 팀원들이 새기술을 이해하도록 독려하고 팀 차원의 베스트 프랙티스 수준을 정하기

### 4.4.2 단순함에 사로잡히다. 
- 증상: 무작정 시스템에서 제일 간단한 부분만 파고든다.
- 현실:
    - 원개발자는 그 시스템 파트를 어떻게 튜닝해야 할 지 안다.
    - 다양한 시스템 컴포넌트에 대해 지식 공유 x, 짝 프로그래밍 x -> 독보적인 전문가만 양산
  - 진단: 시스템을 정상 가동시키는 일이 주임무인, 체계가 잘 잡힌 유지보수 팀에서 흔히 나타남
  - 처방:
    - 측정을 해보고 진짜 성능 병목적을 찾기
    - 보인이 익숙지 않은 컴포넌트에 문제가 생기면 잘 아는 전문가에게 도움을 청하기
    - 개발자가 전체 시스템 컴포넌트를 고루 이해하도록 독려하기

### 4.4.3 성능 튜닝 도사
- 증상: 튜닝에 박식한 전문가를 채용해 사내 모든 성능 이슈를 바로잡고 회사를 정상 궤도에 올려놓으라고 시킴
- 현실: 명망 높은 마법사나 초인이 하는 일이라곤 드레스 코드를 도전하는 일이 고작
- 진단: 스스로 성능 이슈를 해결하기에 실력이 조금 부족하다고 여기는 개발 팀원들이 소외감을 느끼게 하고, 성능 전문가는 결국 성능 지표를 측정하고 그 결괏값을 바탕으로 문제를 해결하는 사람이기에 특정 이슈를 해결한 방법이나 자기가 알고 있는 걸 절대로 남과 공유 안 하려는, 초인을 지향하는 팀원은 매우 반생산적이다.
- 처방:
    - 측정을 해보고 진짜 성능 병목점을 찾기
    - 새로 채용된 팀내 전문가가 다른 팀원들과 지식을 공유하고 팀워크를 유지할 수 있게 리드

### 4.4.4 민간 튜닝
- 증상: 인터넷에 글 쓴 사람이 성능이 향상된다고 장담했으니 테스트도 안 해보고 곧장 마법의 매개변수를 운영 서버에 적용해봄
- 현실:
    - 개발자는 성능 팁의 전후 맥락이나 기초를 모르고 그 팁이 진짜 어떤 영향을 미칠지 모른다.
    - 어떤 시스템에선 통했을지 모르지만 다른 시스템에 적용해도 효함이 있을지는 모를 일이다.
- 진단: 자바 성능은 오만 가지 잡다한 요인이 영향을 미치는 맥락에서 발생한다. 실행 환경이 복잡하기에 전후 맥락을 벚겨낸 나머지만 갖고는 거의 추론할 수가 없음
- 처방:
    - 시스템의 가장 중요한 부분에 직접 영향을 미치는 기술은 확실히 파악하고 충분히 검증된 것들만 적용하기
    - 어떤 변화라도 철저히 검증하고 효용을 프로파일링해야 함
    - 다른 개발자나 운영 요원, 데브옵스티과 함께 설정 문제를 리뷰하고 토의해야 함

### 4.4.5 안되면 조상 탓
- 증상: 정작 이슈와는 아무 상관도 없는 특정 컴포넌트를 문제 삼는다.
- 현실:
    - 충분히 분석도 안 해보고 성급한 결론을 내린다.
    - 평소 의심했던 범인을 수사 과정의 유일한 용의자로 지목하는 것과 마찬가지
- 진단: 기술 스택을 잘 모르는 사람들은 대부분 인지편향을 갖고 있어서 어떤 패턴에 맞춰 진행 -> 아무래도 뭔가 새로 조사하는 것보다는 보통 문제를 많이 일으키는 곳을 지목함
- 처방:
    - 성급한 결론을 내리고픈 욕망에 굴하지 말기
    - 정상적으로 분석을 하기
    - 분석 결과를 모든 이해관계자와 의논

### 4.4.6 숲을 못 보고 나무만 본다.
- 증상: 전체적인 변경 영향도를 완전히 파악하지 않은 채 일단 그냥 변경을 해보거나 애플리케이션의 국소적인 부분만 프로파일링한다.
- 현실:
    - 변경 영향도를 완전히 이해한 사람이 아무도 없음
    - 새로 바꾼 JVM 설정값 하에서 애플리케이션을 완전히 프로파일링하지 않았음
    - 마이크로벤치마킹 때문에 빚어질 전체 시스템 영향도를 파악하지 않음
- 진단: 운영계를 그대로 모사한 UAT 환경 없이 최적화의 효용성을 판단하기 어렵고 부하가 높을 때만 도움이 되고 평소에는 외려 성능을 떨어뜨리는 최적화는 아무 의미가 없음
- 처방: 운영계에서 스위치를 변경하기 전 다음 절차를 따른다.
```
1. 운영계 성능 지표를 측정한다.
2. UAT에서 한번에 스위치 하나씩 변경한다.
3. 스트레스를 받는 지점에 UAT와 운영계가 동일한지 확인한다.
4. 운영계에서 일반적인 부하를 나타내는 테스트 데이터를 확보한다.
5. UAT에서 스위치를 변경하며 테스트한다.
6. UAT에서 다시 테스트한다.
7. 추론한 내용을 다른 사람에게 재검토 요청한다.
8. 내린 결론을 다른 사람과 공유한다.
```

### 4.4.7 내 데스크톱이 UAT
- 증상: UAT환경이 운영계 환경과 전혀 다른 경우도 많으나 개발밪는 그 차이점을 예상하거나 완전히 이해하지 못한 채 저성능 데스크톱에서 고성능 운영 서버로 서비스할 코드를 작성한다.
- 현실: UAT 환경이 운영계와 달라 서비스가 중단되는 사태가 벌어지면 장비 몇 대 더 추가하는 비용보다 훨씬 더 값비싼 대가를 치르게 된다.
- 진단: 유의미한 추정을 하려면 UAT 환경을 운영계와 반드시 동기화해야 한다.
- 처방:
    - 서비스 중단 비용과 고객 이탈로 인한 기회비용을 잘 따져보기
    - 운영 환경과 동일한 UAT 환경을 구입하기
    - 소 잃고 외양간 고치는 비용이 더 많이 들테니, 관리자들에게 올바른 사례를 제시

### 4.4.8 운영 데이터처럼 만들기는 어려워
- 증상: 운영계와 유사한 데이터를 나타내고자 할 때 빠지는 함정, UAT에서 데이터셋을 단순화하여 테스트하는 경우가 많은데 그만큼 쓸 만한 결과를 얻을 확률은 떨어진다.
- 현실: UAT에서 정확한 결과를 얻으려면 운영계 데이터와 최대한 맞추어야 한다. 보안 정책 때문에 원본 데이터를 가져올 수 업사면 데이터를 뒤죽박죽 섞어서 테스트하는 방법도 있다.
- 진단: 낡은 데이터, 비전형적인 데이터라도 테스트를 해보는 게 아예 안 하는 것보다 낫겠지, 싶겠지만 막상 시스템을 오픈하고 UAT 데이터로 예상했던 것과는 전혀 다른 패턴을 보이게 됨
- 처방:
    - 데이터 도메인 전문가에게 컨설팅을 받고 운영 데이터를 UAT로 다시 이전하는 프로세스에 시간과 노력을 투자하기
    - 다수의 고객이 몰리고 엄청난 트랜잭션이 예상되는 서비스는 출시 전 철저하기 준비하기

## 4.5 인지 편향과 성능 테스트 
### 4.5.1 환원주의
- 시스템을 아주 작은 조각으로 나누어 그 구성 파트를 이해하면 전체 시스템도 다 이해할 수 있다는 분석주의적 사고방식
- 그러나 복잡한 시스템은 단순히 구성 파트를 합한 것보다 시스템을 전체로 바라봐야 문제의 원인을 찾을 수 있다.

### 4.5.2 확증 편향
- 테스트 세트를 부실하게 선택하거나 테스트 결과를 통계적으로 건전하게 분석하지 않으면 발현되는 편향
- 강력한 동기가 부여되거나 감정 요소가 개입되기 때문에 거스르기가 어려움

### 4.5.3 행동 편향
- 시스템이 예상대로 작동하지 않는 상황, 또는 아예 중단된 시간 중에 발현되는 편향
- 원인
    - 영향도를 분석해보지도 않고, 다른 사람에게 연락도 안하고 시스템 인프라를 변경
    - 시스템이 의존하는 라이브러리를 변경
    - 연중 가장 업무가 빠듯한 날에 처음 보는 버그나 경합 조건이 발생
- 실패 시나리오를 충분히 테스트 안 해본 상황에서는 시스템 중단을 해결하려고 뭐라도 해야 한다는 막연한 느낌으로 서두르기만 하면 안됨
- 업무 담당자 모두 체계적으로 문제에 접근하는 태도를 가져야 함

### 4.5.4 위험 편향
- 보통 애플리케이션 문제가 생겼을 때 제대로 학습하고 적절한 조치를 하지 못한 까닭에 더 고착화한다.
- 미리 계산된 위험을 조금만 안고 가면 제품을 내놓을 수 있을 때도 사람들의 이런 성향 탓에 상황은 매우 절망적으로 흘러간다.
- 단위테스트 세트와 운영계 회귀 테스트 체계만 확실히 갖추고 나면 리스크를 상당히 줄일 수 있다.


### 4.5.5 엘스버그 역설
- 인간이 확률을 이해하는 데 얼마나 서투른지 잘 보여주는 사례로, '알려지지 않은 미지의 것'보다 '알려진 기지의 것'을 추구하는 인간의 본연의 욕구에 관한 역설이다.

--- 
## + ) 성능 안티패턴 추가 조사

### 성능 테스트를 너무 늦게 시작하기 
성능 테스트를 제품 개발 사이클의 마지막 부분에 집중하면, 성능 문제를 해결하기 위해 전체 설계를 수정해야 하는 상황에 놓일 수 있다. 이는 많은 시간과 노력을 필요로 하며, 결과적으로 프로젝트의 비용과 일정에 부정적인 영향을 미칠 수 있다.

### 부정확한 로드 테스트 
실제 사용자 행동과 유사한 부하를 생성하지 않으면, 성능 테스트의 결과는 실제 운영 환경에서의 성능을 정확하게 반영하지 못할 수 있다. 사용자 행동을 정확하게 모델링하고, 그에 따른 부하를 생성하는 것이 중요하다.

### 성능 개선의 효과를 측정하지 않기 
성능 개선 조치를 취한 후, 그 효과를 측정하지 않으면 어떤 조치가 실제로 성능에 긍정적인 영향을 미치는지 알 수 없다. 모든 개선 조치를 적용한 후에는 성능 테스트를 반복하여 그 효과를 측정해야 한다.

### 만능 해결책에 의존하기 
성능 문제는 복잡하고 다양한 요인들에 의해 발생하므로, 한 가지 해결책이 모든 문제를 해결할 수 있을 것이라는 생각은 위험하다. 성능 문제를 해결하기 위해서는 시스템 전체를 이해하고, 여러 가지 방법을 적용하며 지속적으로 테스트해야 한다.

### 소프트웨어 변경 없이 하드웨어만으로 성능을 향상시키려고 시도하기 
하드웨어를 업그레이드하는 것은 비용이 많이 들 수 있으며, 항상 성능 문제를 해결하지는 못한다. 대부분의 경우, 성능 문제는 소프트웨어 설계나 구현의 문제로부터 기인하기 때문에, 코드의 최적화나 아키텍처의 개선이 더욱 효과적일 수 있다.

### 성능 테스트의 자동화를 무시하기 
성능 테스트를 수동으로 실행하면 많은 시간과 노력이 필요하며, 실수의 여지도 많다. 성능 테스트를 자동화하면 테스트의 일관성을 유지할 수 있고, 개발팀이 더 빠르게 피드백을 받아 성능 개선에 투자할 수 있다.

